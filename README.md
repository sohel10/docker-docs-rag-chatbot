
# ğŸ“¦ Docker Docs RAG Chatbot

A production-style Retrieval-Augmented Generation (RAG) system that answers questions strictly from Docker documentation and security PDFs, with transparent source attribution.
Built with FastAPI, Streamlit, FAISS, Ollama, and Docker Compose, this project demonstrates end-to-end AI system design, from document ingestion to API + UI deployment. 
The system answers questions **strictly from provided documents**, with
transparent source attribution.
# ğŸš€ Why This Project Matters

# This repository is designed to showcase real-world AI engineering skills

âœ… Document-grounded answers 

âœ… API-first architecture

âœ… Containerized deployment

âœ… Local LLM inference (privacy-preserving)

âœ… Recruiter-ready, production layout


## ğŸš€ Features

- ğŸ“„ Supports **Markdown (.md)** and **PDF** documents
- ğŸ” Semantic search with **FAISS**
- ğŸ§  Local LLM inference using **Ollama (LLaMA 3)**
- ğŸ’¬ Interactive **Streamlit chat UI**
- ğŸ” Source citations for every answer
- ğŸ”’ Fully local / private (no OpenAI or external APIs)


## ğŸ§± Architecture

### FastAPI Backend
<p align="center">
  <img src="api.png" width="85%">
  <br/>
  <em>FastAPI service exposing document-grounded RAG inference</em>
</p>

### Streamlit UI
<p align="center">
  <img src="streamlit.png" width="85%">
  <br/>
  <em>Interactive chat UI for querying Docker documentation</em>
</p>

### RAG Pipeline
<p align="center">
  <img src="rag.png" width="70%">
  <br/>
  <em>End-to-end Retrieval-Augmented Generation workflow</em>
</p>

**Data Flow:** Documents â†’ Chunking â†’ Embeddings â†’ FAISS â†’ FastAPI â†’ Streamlit

# Why this works (important)

api.png, streamlit.png, rag.png are in repo root

GitHub README automatically resolves relative paths

<p align="center"> keeps it clean and recruiter-friendly

Width keeps it readable on laptop & mobile


## ğŸ“ Project Structure

```text
docker-docs-rag-chatbot/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_docs/                 # Markdown & PDF source documents
â”‚   â”‚   â”œâ”€â”€ docker_build.md
â”‚   â”‚   â”œâ”€â”€ docker_compose.md
â”‚   â”‚   â”œâ”€â”€ docker_engine.md
â”‚   â”‚   â””â”€â”€ NIST.SP.800-190.pdf
â”‚   â”‚
â”‚   â”œâ”€â”€ processed/                # Intermediate artifacts
â”‚   â”‚   â””â”€â”€ chunks.json
â”‚   â”‚
â”‚   â””â”€â”€ vectorstore/faiss/        # FAISS vector index
â”‚       â”œâ”€â”€ index.faiss
â”‚       â””â”€â”€ index.pkl
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingest.py                 # Extract text from docs
â”‚   â”œâ”€â”€ embed_faiss.py            # Create embeddings + FAISS index
â”‚   â”œâ”€â”€ rag_core.py               # RAG engine (retriever + LLM)
â”‚   â”œâ”€â”€ api.py                    # FastAPI backend
â”‚   â””â”€â”€ streamlit_app.py          # Streamlit chat UI
â”‚
â”œâ”€â”€ Dockerfile.api                # FastAPI container
â”œâ”€â”€ Dockerfile.streamlit          # Streamlit UI container
â”œâ”€â”€ docker-compose.yml            # API + UI + Ollama orchestration
â”‚
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ README.md                     # Project documentation
â””â”€â”€ LICENSE





## âš™ï¸ Setup

### 1ï¸âƒ£ Create environment

```bash
conda create -n rag-chatbot python=3.10
conda activate rag-chatbot
pip install -r requirements.txt

# Install & start Ollama
data/raw_docs/
python src/ingest.py
python src/embed_faiss.py

# Run the Chatbot
streamlit run src/streamlit_app.py

# MIT License


âœ… **Commit this first**  
This alone already makes your repo look serious.



# âœ… STEP 2 â€” Add FastAPI Backend

## ğŸš€ Deployment

Designed for production-style deployment on **AWS EC2** using Docker Compose.

- API (FastAPI) and UI (Streamlit) run as separate services
- Ollama runs locally inside a container for private LLM inference
- Services can be scaled independently

```bash
docker compose up --build
Access:

API Docs â†’ http://localhost:8000/docs

Chat UI â†’ http://localhost:8501



ğŸ”¹ Install system deps
sudo apt update
sudo apt install -y python3-pip git


Install Ollama:

curl -fsSL https://ollama.com/install.sh | sh
ollama run llama3

ğŸ”¹ Deploy project
git clone https://github.com/yourusername/rag-chatbot.git
cd rag-chatbot
pip install -r requirements.txt


Run API:

uvicorn src.api:app --host 0.0.0.0 --port 8000


Run Streamlit:

streamlit run src/streamlit_app.py --server.port 8501 --server.address 0.0.0.0
=======
# docker-docs-rag-chatbot
