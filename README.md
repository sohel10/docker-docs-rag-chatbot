
# ğŸ“¦ Docker Docs RAG Chatbot

A production-style **Retrieval-Augmented Generation (RAG)** chatbot built using
Docker documentation and security PDFs.  
The system answers questions **strictly from provided documents**, with
transparent source attribution.

---

## ğŸš€ Features

- ğŸ“„ Supports **Markdown (.md)** and **PDF** documents
- ğŸ” Semantic search with **FAISS**
- ğŸ§  Local LLM inference using **Ollama (LLaMA 3)**
- ğŸ’¬ Interactive **Streamlit chat UI**
- ğŸ” Source citations for every answer
- ğŸ”’ Fully local / private (no OpenAI or external APIs)

---

## ğŸ§± Architecture

Documents (.md / .pdf)
â†“
Text Extraction
â†“
Chunking
â†“
Embeddings (Sentence Transformers)
â†“
FAISS Vector Store
â†“
Retriever
â†“
Ollama LLM
â†“
Answer + Sources


---

## ğŸ“‚ Project Structure

rag-chatbot/
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw_docs/ # Markdown & PDF files
â”‚ â”œâ”€â”€ processed/
â”‚ â”‚ â””â”€â”€ chunks.json
â”‚ â””â”€â”€ vectorstore/faiss/
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ ingest.py
â”‚ â”œâ”€â”€ embed_faiss.py
â”‚ â”œâ”€â”€ rag_chat.py
â”‚ â””â”€â”€ streamlit_app.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


---

## âš™ï¸ Setup

### 1ï¸âƒ£ Create environment

```bash
conda create -n rag-chatbot python=3.10
conda activate rag-chatbot
pip install -r requirements.txt

# Install & start Ollama
data/raw_docs/
python src/ingest.py
python src/embed_faiss.py

# Run the Chatbot
streamlit run src/streamlit_app.py

# MIT License


âœ… **Commit this first**  
This alone already makes your repo look serious.



# âœ… STEP 2 â€” Add FastAPI Backend

This allows:
- API usage
- Cloud deployment
- CI testing
- Separation of UI and logic

## Deploy to AWS EC2
EC2 Setup (one time)

Instance: t3.medium

OS: Ubuntu 22.04

Storage: 30â€“50 GB

Open ports:

22 (SSH)

8000 (API)

8501 (Streamlit)

ğŸ”¹ SSH into EC2
ssh ubuntu@<EC2_PUBLIC_IP>

ğŸ”¹ Install system deps
sudo apt update
sudo apt install -y python3-pip git


Install Ollama:

curl -fsSL https://ollama.com/install.sh | sh
ollama run llama3

ğŸ”¹ Deploy project
git clone https://github.com/yourusername/rag-chatbot.git
cd rag-chatbot
pip install -r requirements.txt


Run API:

uvicorn src.api:app --host 0.0.0.0 --port 8000


Run Streamlit:

streamlit run src/streamlit_app.py --server.port 8501 --server.address 0.0.0.0
=======
# docker-docs-rag-chatbot