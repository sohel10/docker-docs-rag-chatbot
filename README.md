# ğŸ“¦ Docker Docs RAG Chatbot

## ğŸš€ FastAPI Backend

<p align="center">
  <img src="./api.png" alt="FastAPI RAG API" width="900"/>
</p>

<p align="center">
  <em>FastAPI service exposing document-grounded RAG inference</em>
</p>


A production-style Retrieval-Augmented Generation (RAG) system that answers questions strictly from Docker documentation and security PDFs, with transparent source attribution.
Built with FastAPI, Streamlit, FAISS, Ollama, and Docker Compose, this project demonstrates end-to-end AI system design, from document ingestion to API + UI deployment. 
The system answers questions **strictly from provided documents**, with
transparent source attribution.
# ğŸš€ Why This Project Matters

# This repository is designed to showcase real-world AI engineering skills

âœ… Document-grounded answers 

âœ… API-first architecture

âœ… Containerized deployment

âœ… Local LLM inference (privacy-preserving)

âœ… Recruiter-ready, production layout


## ğŸš€ Features

## ğŸ’¬ Streamlit Chat UI

<p align="center">
  <img src="streamlit.png" alt="Streamlit RAG Chat UI" width="900"/>
</p>

<p align="center">
  <em>Interactive chat UI for querying Docker documentation with source attribution</em>
</p>

- ğŸ“„ Supports **Markdown (.md)** and **PDF** documents
- ğŸ” Semantic search with **FAISS**
- ğŸ§  Local LLM inference using **Ollama (LLaMA 3)**
- ğŸ’¬ Interactive **Streamlit chat UI**
- ğŸ” Source citations for every answer
- ğŸ”’ Fully local / private (no OpenAI or external APIs)



## ğŸ§± RAG Architecture

<p align="center">
  <img src="rag.png" alt="RAG Pipeline Architecture" width="300"/>
</p>

<p align="center">
  <em>
    Document ingestion â†’ embeddings â†’ FAISS retrieval â†’ Ollama (LLaMA 3) â†’ FastAPI â†’ Streamlit UI
  </em>
</p>


# Why this works (important)

api.png, streamlit.png, rag.png are in repo root

GitHub README automatically resolves relative paths

<p align="center"> keeps it clean and recruiter-friendly

Width keeps it readable on laptop & mobile


## ğŸ“ Project Structure

## ğŸ“‚ Project Structure

```text
docker-docs-rag-chatbot/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_docs/                 # Markdown & PDF source documents
â”‚   â”œâ”€â”€ processed/                # Intermediate artifacts
â”‚   â”‚   â””â”€â”€ chunks.json
â”‚   â””â”€â”€ vectorstore/faiss/         # FAISS vector index
â”‚       â”œâ”€â”€ index.faiss
â”‚       â””â”€â”€ index.pkl
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingest.py                 # Extract text from docs
â”‚   â”œâ”€â”€ embed_faiss.py            # Create embeddings + FAISS index
â”‚   â”œâ”€â”€ rag_core.py               # RAG engine (retriever + LLM)
â”‚   â”œâ”€â”€ api.py                    # FastAPI backend
â”‚   â””â”€â”€ streamlit_app.py          # Streamlit chat UI
â”‚
â”œâ”€â”€ Dockerfile.api                # FastAPI container
â”œâ”€â”€ Dockerfile.streamlit          # Streamlit UI container
â”œâ”€â”€ docker-compose.yml            # API + UI + Ollama orchestration
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE



## âš™ï¸ Setup

### 1ï¸âƒ£ Create environment

```bash
conda create -n rag-chatbot python=3.10
conda activate rag-chatbot
pip install -r requirements.txt

# Install & start Ollama
data/raw_docs/
python src/ingest.py
python src/embed_faiss.py

# Run the Chatbot
streamlit run src/streamlit_app.py

# MIT License


âœ… **Commit this first**  
This alone already makes your repo look serious.



# âœ… STEP 2 â€” Add FastAPI Backend

## ğŸš€ Deployment

Designed for production-style deployment on **AWS EC2** using Docker Compose.

- API (FastAPI) and UI (Streamlit) run as separate services
- Ollama runs locally inside a container for private LLM inference
- Services can be scaled independently

```bash
docker compose up --build
Access:

API Docs â†’ http://localhost:8000/docs

Chat UI â†’ http://localhost:8501



ğŸ”¹ Install system deps
sudo apt update
sudo apt install -y python3-pip git


Install Ollama:

curl -fsSL https://ollama.com/install.sh | sh
ollama run llama3

ğŸ”¹ Deploy project
git clone https://github.com/yourusername/rag-chatbot.git
cd rag-chatbot
pip install -r requirements.txt


Run API:

## âœ… 5ï¸âƒ£ Add a short â€œDeploymentâ€ visual explanation

Under **Deployment** section:

```md
## ğŸš€ Deployment (Docker Compose + AWS EC2)


- API (FastAPI) and UI (Streamlit) run as separate services
- Ollama runs locally inside a container for private LLM inference
- Services can be scaled independently
