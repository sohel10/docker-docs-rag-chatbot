
# ğŸ“¦ Docker Docs RAG Chatbot

A production-style Retrieval-Augmented Generation (RAG) system that answers questions strictly from Docker documentation and security PDFs, with transparent source attribution.
Built with FastAPI, Streamlit, FAISS, Ollama, and Docker Compose, this project demonstrates end-to-end AI system design, from document ingestion to API + UI deployment. 
The system answers questions **strictly from provided documents**, with
transparent source attribution.
# ğŸš€ Why This Project Matters

# This repository is designed to showcase real-world AI engineering skills

âœ… Document-grounded answers 

âœ… API-first architecture

âœ… Containerized deployment

âœ… Local LLM inference (privacy-preserving)

âœ… Recruiter-ready, production layout


## ğŸš€ Features

- ğŸ“„ Supports **Markdown (.md)** and **PDF** documents
- ğŸ” Semantic search with **FAISS**
- ğŸ§  Local LLM inference using **Ollama (LLaMA 3)**
- ğŸ’¬ Interactive **Streamlit chat UI**
- ğŸ” Source citations for every answer
- ğŸ”’ Fully local / private (no OpenAI or external APIs)


## ğŸ§± Architecture

Documents (.md / .pdf)
â†“
Text Extraction
â†“
Chunking
â†“
Embeddings (Sentence Transformers)
â†“
FAISS Vector Store
â†“
Retriever
â†“
Ollama LLM
â†“
Answer + Sources


---

## ğŸ“‚ Project Structure

docker-docs-rag-chatbot/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_docs/              # Markdown & PDF source documents
â”‚   â”‚   â”œâ”€â”€ docker_build.md
â”‚   â”‚   â”œâ”€â”€ docker_compose.md
â”‚   â”‚   â”œâ”€â”€ docker_engine.md
â”‚   â”‚   â””â”€â”€ NIST.SP.800-190.pdf
â”‚   â”‚
â”‚   â”œâ”€â”€ processed/             # Intermediate artifacts
â”‚   â”‚   â””â”€â”€ chunks.json        # Chunked document text
â”‚   â”‚
â”‚   â””â”€â”€ vectorstore/faiss/     # FAISS vector index
â”‚       â”œâ”€â”€ index.faiss
â”‚       â””â”€â”€ index.pkl
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingest.py              # Extract text from docs
â”‚   â”œâ”€â”€ embed_faiss.py         # Create embeddings + FAISS index
â”‚   â”œâ”€â”€ rag_core.py            # RAG engine (retriever + LLM)
â”‚   â”œâ”€â”€ api.py                 # FastAPI backend
â”‚   â””â”€â”€ streamlit_app.py       # Streamlit chat UI
â”‚
â”œâ”€â”€ Dockerfile.api              # FastAPI container
â”œâ”€â”€ Dockerfile.streamlit        # Streamlit UI container
â”œâ”€â”€ docker-compose.yml          # Orchestrates API + UI + Ollama
â”‚
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ README.md                   # Project documentation
â””â”€â”€ LICENSE



---

## âš™ï¸ Setup

### 1ï¸âƒ£ Create environment

```bash
conda create -n rag-chatbot python=3.10
conda activate rag-chatbot
pip install -r requirements.txt

# Install & start Ollama
data/raw_docs/
python src/ingest.py
python src/embed_faiss.py

# Run the Chatbot
streamlit run src/streamlit_app.py

# MIT License


âœ… **Commit this first**  
This alone already makes your repo look serious.



# âœ… STEP 2 â€” Add FastAPI Backend

This allows:
- API usage
- Cloud deployment
- CI testing
- Separation of UI and logic

## Deploy to AWS EC2
EC2 Setup (one time)

Instance: t3.medium

OS: Ubuntu 22.04

Storage: 30â€“50 GB

Open ports:

22 (SSH)

8000 (API)

8501 (Streamlit)

ğŸ”¹ SSH into EC2
ssh ubuntu@<EC2_PUBLIC_IP>

ğŸ”¹ Install system deps
sudo apt update
sudo apt install -y python3-pip git


Install Ollama:

curl -fsSL https://ollama.com/install.sh | sh
ollama run llama3

ğŸ”¹ Deploy project
git clone https://github.com/yourusername/rag-chatbot.git
cd rag-chatbot
pip install -r requirements.txt


Run API:

uvicorn src.api:app --host 0.0.0.0 --port 8000


Run Streamlit:

streamlit run src/streamlit_app.py --server.port 8501 --server.address 0.0.0.0
=======
# docker-docs-rag-chatbot
